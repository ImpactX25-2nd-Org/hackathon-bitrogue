# Multi-Language Support Setup (English, Tamil, Kannada)

## ğŸŒ Features Implemented

### Backend Services
1. **Translation Service** - IndicTrans2 for English â†” Tamil â†” Kannada
2. **Audio Service** - Whisper for speech-to-text in all 3 languages
3. **Auto-translation** - AI advice automatically translated to user's language

### Frontend
- Language selector updated to show only: English, Tamil, Kannada
- All UI will auto-translate based on selected language

## ğŸ“¦ Installation Steps

### 1. Install Python Dependencies

```bash
cd backend
pip install transformers==4.36.0
pip install IndicTransToolkit
pip install sentencepiece==0.1.99
pip install openai-whisper
pip install ffmpeg-python
```

### 2. Install FFmpeg (Required for Whisper)

**Windows:**
```powershell
# Download from: https://github.com/BtbN/FFmpeg-Builds/releases
# Or use chocolatey:
choco install ffmpeg
```

**Mac:**
```bash
brew install ffmpeg
```

**Linux:**
```bash
sudo apt update
sudo apt install ffmpeg
```

### 3. Hugging Face Login (for IndicTrans2)

```python
from huggingface_hub import login
login(token="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")
```

Or set environment variable:
```bash
export HF_TOKEN=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
```

## ğŸš€ Usage

### Backend API Endpoints

#### 1. Translate Text
```http
POST /api/language/translate
Content-Type: multipart/form-data

texts: ["Hello", "How are you?"]
src_lang: en
tgt_lang: ta
```

Response:
```json
{
  "success": true,
  "data": {
    "translations": ["à®µà®£à®•à¯à®•à®®à¯", "à®¨à¯€à®™à¯à®•à®³à¯ à®à®ªà¯à®ªà®Ÿà®¿ à®‡à®°à¯à®•à¯à®•à®¿à®±à¯€à®°à¯à®•à®³à¯?"],
    "src_lang": "en",
    "tgt_lang": "ta",
    "count": 2
  }
}
```

#### 2. Transcribe Audio
```http
POST /api/language/transcribe
Content-Type: multipart/form-data

audio: <audio file>
language: ta  (optional - auto-detects if not provided)
```

Response:
```json
{
  "success": true,
  "data": {
    "text": "à®µà®£à®•à¯à®•à®®à¯ à®¨à®¾à®©à¯ à®’à®°à¯ à®µà®¿à®µà®šà®¾à®¯à®¿",
    "language": "tamil",
    "filename": "recording.mp3"
  }
}
```

#### 3. Get Supported Languages
```http
GET /api/language/supported-languages
```

Response:
```json
{
  "success": true,
  "data": {
    "languages": [
      {"code": "en", "name": "English", "native": "English"},
      {"code": "ta", "name": "Tamil", "native": "à®¤à®®à®¿à®´à¯"},
      {"code": "kn", "name": "Kannada", "native": "à²•à²¨à³à²¨à²¡"}
    ]
  }
}
```

### Automatic Translation

When uploading a scan, the AI advice will automatically be translated to the user's selected language:

```http
POST /api/scans
Content-Type: multipart/form-data

image: <crop image>
crop_type: rice
language: ta  <-- AI advice will be in Tamil
```

## ğŸ¯ How It Works

### AI Advice Flow
1. **ML Detection** â†’ Detects disease (English)
2. **RAG Retrieval** â†’ Gets knowledge base info (English)
3. **LLM Generation** â†’ Generates treatment advice (English)
4. **Translation** â†’ Translates to user's language (ta/kn if selected)
5. **Response** â†’ Returns translated advice

### Translation Service
- Uses **IndicTrans2-200M** models (lightweight, fast)
- Supports direct translation: en â†” ta, en â†” kn
- For ta â†” kn, uses English as pivot language
- **Lazy loading**: Models load only when first used

### Audio Service
- Uses **Whisper-base** model (good balance of speed/accuracy)
- Auto-detects language if not specified
- Supports common audio formats: mp3, wav, m4a, ogg
- Returns timestamped segments for advanced features

## ğŸ”§ Optimization Tips

### Model Sizes

**IndicTrans2:**
- `200M` (current) - Fast, good for real-time
- `1.2B` - Better quality, slower

**Whisper:**
- `tiny` - Fastest, lower accuracy
- `base` (current) - Balanced
- `small` - Better accuracy
- `medium/large` - Best quality, GPU recommended

### GPU Acceleration
If you have CUDA GPU:
```python
# Translation service automatically uses GPU if available
device = "cuda" if torch.cuda.is_available() else "cpu"
```

### Caching
- Translation results are NOT cached (real-time translation)
- LLM responses ARE cached for 24h per disease+language
- Whisper transcriptions are NOT cached (each audio unique)

## ğŸ“ Files Created/Modified

### Backend
- `app/services/translation_service.py` - IndicTrans2 integration
- `app/services/audio_service.py` - Whisper integration  
- `app/services/llm_service.py` - Added auto-translation
- `app/routes/language.py` - Translation & audio APIs
- `app/main.py` - Service initialization
- `requirements.txt` - Added new dependencies

### Frontend
- `src/contexts/LanguageContext.tsx` - Updated to 3 languages only

## ğŸ› Troubleshooting

### Issue: "No module named 'IndicTransToolkit'"
```bash
pip install git+https://github.com/VarunGumma/IndicTransToolkit.git
```

### Issue: Whisper FFmpeg error
```bash
# Make sure FFmpeg is in PATH
ffmpeg -version
```

### Issue: Out of memory
```python
# Use smaller models in audio_service.py:
audio_service = AudioService(model_size="tiny")  # Instead of "base"
```

### Issue: Slow translation
```python
# Models load on first use. First translation will be slow.
# Subsequent translations are fast.
# To pre-load on startup, add to main.py:
translation_service.translation_service._load_en_to_indic()
translation_service.translation_service._load_indic_to_en()
```

## âœ… Testing

Run test to verify setup:
```bash
python test_indictrans.py
```

Expected output:
```
âœ“ Model loaded successfully!
English: Hello, how are you?
Tamil  : à®µà®£à®•à¯à®•à®®à¯, à®¨à¯€à®™à¯à®•à®³à¯ à®à®ªà¯à®ªà®Ÿà®¿ à®‡à®°à¯à®•à¯à®•à®¿à®±à¯€à®°à¯à®•à®³à¯?
âœ“ All tests completed successfully!
```

## ğŸ‰ Done!

Now your entire application supports:
- âœ… English, Tamil, Kannada site-wide
- âœ… AI advice auto-translated to user language
- âœ… Community posts translatable
- âœ… Audio input for farmers
- âœ… Voice-to-text search and queries
